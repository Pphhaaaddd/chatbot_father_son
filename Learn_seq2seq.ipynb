{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "from random import randint\n",
    "import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes an annoying Tensorflow warning\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "def createTrainingMatrices(conversationFileName, wList, maxLen):\n",
    "    conversationDictionary = np.load(conversationFileName).item()\n",
    "    numExamples = len(conversationDictionary)\n",
    "    xTrain = np.zeros((numExamples, maxLen), dtype='int32')\n",
    "    yTrain = np.zeros((numExamples, maxLen), dtype='int32')\n",
    "    for index,(key,value) in enumerate(conversationDictionary.items()):\n",
    "        # Will store integerized representation of strings here (initialized as padding)\n",
    "        encoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
    "        decoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
    "        # Getting all the individual words in the strings\n",
    "        keySplit = key.split()\n",
    "        valueSplit = value.split()\n",
    "        keyCount = len(keySplit)\n",
    "        valueCount = len(valueSplit)\n",
    "        # Throw out sequences that are too long or are empty\n",
    "        if (keyCount > (maxLen - 1) or valueCount > (maxLen - 1) or valueCount == 0 or keyCount == 0):\n",
    "            continue\n",
    "        # Integerize the encoder string\n",
    "        for keyIndex, word in enumerate(keySplit):\n",
    "            try:\n",
    "                encoderMessage[keyIndex] = wList.index(word)\n",
    "            except ValueError:\n",
    "                # TODO: This isnt really the right way to handle this scenario\n",
    "                encoderMessage[keyIndex] = 0\n",
    "        encoderMessage[keyIndex + 1] = wList.index('<EOS>')\n",
    "        # Integerize the decoder string\n",
    "        for valueIndex, word in enumerate(valueSplit):\n",
    "            try:\n",
    "                decoderMessage[valueIndex] = wList.index(word)\n",
    "            except ValueError:\n",
    "                decoderMessage[valueIndex] = 0\n",
    "        decoderMessage[valueIndex + 1] = wList.index('<EOS>')\n",
    "        xTrain[index] = encoderMessage\n",
    "        yTrain[index] = decoderMessage\n",
    "    # Remove rows with all zeros\n",
    "    yTrain = yTrain[~np.all(yTrain == 0, axis=1)]\n",
    "    xTrain = xTrain[~np.all(xTrain == 0, axis=1)]\n",
    "    numExamples = xTrain.shape[0]\n",
    "\n",
    "    return numExamples, xTrain, yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingBatch(localXTrain, localYTrain, localBatchSize, maxLen):\n",
    "    num = randint(0,numTrainingExamples - localBatchSize - 1)\n",
    "    arr = localXTrain[num:num + localBatchSize]\n",
    "    labels = localYTrain[num:num + localBatchSize]\n",
    "    # Reversing the order of encoder string apparently helps as per 2014 paper\n",
    "    reversedList = list(arr)\n",
    "    for index,example in enumerate(reversedList):\n",
    "        reversedList[index] = list(reversed(example))\n",
    "\n",
    "    # Lagged labels are for the training input into the decoder\n",
    "    laggedLabels = []\n",
    "    EOStokenIndex = wordList.index('<EOS>')\n",
    "    padTokenIndex = wordList.index('<pad>')\n",
    "    for example in labels:\n",
    "        eosFound = np.argwhere(example==EOStokenIndex)[0]\n",
    "        shiftedExample = np.roll(example,1)\n",
    "        shiftedExample[0] = EOStokenIndex\n",
    "        # The EOS token was already at the end, so no need for pad\n",
    "        if (eosFound != (maxLen - 1)):\n",
    "            shiftedExample[eosFound+1] = padTokenIndex\n",
    "        laggedLabels.append(shiftedExample)\n",
    "\n",
    "    # Need to transpose these\n",
    "    reversedList = np.asarray(reversedList).T.tolist()\n",
    "    labels = labels.T.tolist()\n",
    "    laggedLabels = np.asarray(laggedLabels).T.tolist()\n",
    "    return reversedList, labels, laggedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint(0,numTrainingExamples - 24 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateToSentences(inputs, wList, encoder=False):\n",
    "    EOStokenIndex = wList.index('<EOS>')\n",
    "    padTokenIndex = wList.index('<pad>')\n",
    "    numStrings = len(inputs[0])\n",
    "    numLengthOfStrings = len(inputs)\n",
    "    listOfStrings = [''] * numStrings\n",
    "    for mySet in inputs:\n",
    "        for index,num in enumerate(mySet):\n",
    "            if (num != EOStokenIndex and num != padTokenIndex):\n",
    "                if (encoder):\n",
    "                    # Encodings are in reverse!\n",
    "                    listOfStrings[index] = wList[num] + \" \" + listOfStrings[index]\n",
    "                else:\n",
    "                    listOfStrings[index] = listOfStrings[index] + \" \" + wList[num]\n",
    "    listOfStrings = [string.strip() for string in listOfStrings]\n",
    "    return listOfStrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestInput(inputMessage, wList, maxLen):\n",
    "    encoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
    "    inputSplit = inputMessage.lower().split()\n",
    "    for index,word in enumerate(inputSplit):\n",
    "        try:\n",
    "            encoderMessage[index] = wList.index(word)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    encoderMessage[index + 1] = wList.index('<EOS>')\n",
    "    encoderMessage = encoderMessage[::-1]\n",
    "    encoderMessageList=[]\n",
    "    for num in encoderMessage:\n",
    "        encoderMessageList.append([num])\n",
    "    return encoderMessageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idsToSentence(ids, wList):\n",
    "    EOStokenIndex = wList.index('<EOS>')\n",
    "    padTokenIndex = wList.index('<pad>')\n",
    "    myStr = \"\"\n",
    "    listOfResponses=[]\n",
    "    for num in ids:\n",
    "        if (num[0] == EOStokenIndex or num[0] == padTokenIndex):\n",
    "            listOfResponses.append(myStr)\n",
    "            myStr = \"\"\n",
    "        else:\n",
    "            myStr = myStr + wList[num[0]] + \" \"\n",
    "    if myStr:\n",
    "        listOfResponses.append(myStr)\n",
    "    listOfResponses = [i for i in listOfResponses if i]\n",
    "    return listOfResponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "batchSize = 24\n",
    "maxEncoderLength = 15\n",
    "maxDecoderLength = maxEncoderLength\n",
    "lstmUnits = 112\n",
    "embeddingDim = lstmUnits\n",
    "numLayersLSTM = 3\n",
    "numIterations = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No embedding matrix found, setting dimensions of word vectors to 100\n"
     ]
    }
   ],
   "source": [
    "# Loading in all the data structures\n",
    "with open(\"data/SonWordList.txt\", \"rb\") as fp:\n",
    "\twordList = pickle.load(fp)\n",
    "\n",
    "vocabSize = len(wordList)\n",
    "\n",
    "# If you've run the entirety of word2vec.py then these lines will load in\n",
    "# the embedding matrix.\n",
    "if (os.path.isfile('embeddingMatrix.npy')):\n",
    "\twordVectors = np.load('embeddingMatrix.npy')\n",
    "\twordVecDimensions = wordVectors.shape[1]\n",
    "else:\n",
    "\tprint('No embedding matrix found, setting dimensions of word vectors to 100')\n",
    "\twordVecDimensions = int(100)\n",
    "\n",
    "# Add two entries to the word vector matrix. One to represent padding tokens,\n",
    "# and one to represent an end of sentence token\n",
    "padVector = np.zeros((1, wordVecDimensions), dtype='int32')\n",
    "EOSVector = np.ones((1, wordVecDimensions), dtype='int32')\n",
    "if (os.path.isfile('embeddingMatrix.npy')):\n",
    "\twordVectors = np.concatenate((wordVectors,padVector), axis=0)\n",
    "\twordVectors = np.concatenate((wordVectors,EOSVector), axis=0)\n",
    "\n",
    "# Need to modify the word list as well\n",
    "wordList.append('<pad>')\n",
    "wordList.append('<EOS>')\n",
    "vocabSize = vocabSize + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading training matrices\n"
     ]
    }
   ],
   "source": [
    "if (os.path.isfile('data/SonSeq2SeqXTrain.npy') and os.path.isfile('data/SonSeq2SeqYTrain.npy')):\n",
    "    xTrain = np.load('data/SonSeq2SeqXTrain.npy')\n",
    "    yTrain = np.load('data/SonSeq2SeqYTrain.npy')\n",
    "    print('Finished loading training matrices')\n",
    "    numTrainingExamples = xTrain.shape[0]\n",
    "else:\n",
    "    numTrainingExamples, xTrain, yTrain = createTrainingMatrices('data/sonConversationDictionary.npy', wordList, maxEncoderLength)\n",
    "    np.save('data/SonSeq2SeqXTrain.npy', xTrain)\n",
    "    np.save('data/SonSeq2SeqYTrain.npy', yTrain)\n",
    "    print('Finished creating training matrices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(15, ?), dtype=int64)\n",
      "Training Checkpoint Found, Loading ... \n",
      "INFO:tensorflow:Restoring parameters from models/son/sonpretrained_seq2seq.ckpt-180000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create the placeholders\n",
    "encoderInputs = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxEncoderLength)]\n",
    "decoderLabels = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxDecoderLength)]\n",
    "decoderInputs = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxDecoderLength)]\n",
    "feedPrevious = tf.placeholder(tf.bool)\n",
    "\n",
    "encoderLSTM = tf.nn.rnn_cell.LSTMCell(lstmUnits, state_is_tuple=True, name='basic_lstm_cell')\n",
    "\n",
    "#encoderLSTM = tf.nn.rnn_cell.MultiRNNCell([singleCell]*numLayersLSTM, state_is_tuple=True)\n",
    "# Architectural choice of of whether or not to include ^\n",
    "\n",
    "decoderOutputs, decoderFinalState = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(encoderInputs, decoderInputs, encoderLSTM,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvocabSize, vocabSize, embeddingDim, feed_previous=feedPrevious)\n",
    "\n",
    "decoderPrediction = tf.argmax(decoderOutputs, 2)\n",
    "\n",
    "lossWeights = [tf.ones_like(l, dtype=tf.float32) for l in decoderLabels]\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(decoderOutputs, decoderLabels, lossWeights, vocabSize)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Loading in a saved model\n",
    "if (os.path.isfile('models/son/checkpoint')):\n",
    "\tprint('Training Checkpoint Found, Loading ... ')\n",
    "\tsaver.restore(sess, tf.train.latest_checkpoint('models/son/'))\n",
    "\n",
    "# Uploading results to Tensorboard\n",
    "tf.summary.scalar('Loss', loss)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "# Some test strings that we'll use as input at intervals during training\n",
    "encoderTestStrings = [\"I know\",\n",
    "\t\t\t\t\t\"Are you sure?\",\n",
    "\t\t\t\t\t\"How are you son?\",\n",
    "\t\t\t\t\t\"remarkable\",\n",
    "\t\t\t\t\t\"Did you hear from Law School\"\n",
    "\t\t\t\t\t]\n",
    "\n",
    "zeroVector = np.zeros((1), dtype='int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hey': 2,\n",
       "         'dad': 13,\n",
       "         \"let's\": 1,\n",
       "         'talk': 7,\n",
       "         'about': 19,\n",
       "         'your': 42,\n",
       "         'childhood': 2,\n",
       "         'i’ve': 3,\n",
       "         'always': 13,\n",
       "         'been': 7,\n",
       "         'curious': 1,\n",
       "         'you': 184,\n",
       "         'know': 50,\n",
       "         'i': 304,\n",
       "         'was': 98,\n",
       "         'never': 23,\n",
       "         'as': 19,\n",
       "         'good': 7,\n",
       "         'of': 63,\n",
       "         'a': 92,\n",
       "         'kid': 9,\n",
       "         'you’ve': 4,\n",
       "         'mentioned': 2,\n",
       "         'think': 26,\n",
       "         'it': 98,\n",
       "         'different': 3,\n",
       "         'growing': 5,\n",
       "         'up': 18,\n",
       "         'in': 38,\n",
       "         'smaller': 1,\n",
       "         'city': 2,\n",
       "         'probably': 6,\n",
       "         'london': 2,\n",
       "         'is': 16,\n",
       "         'bit': 5,\n",
       "         'weird': 2,\n",
       "         'one': 18,\n",
       "         'what': 55,\n",
       "         'way': 11,\n",
       "         'don’t': 15,\n",
       "         'sort': 6,\n",
       "         'get': 28,\n",
       "         'used': 10,\n",
       "         'to': 123,\n",
       "         'having': 4,\n",
       "         'so': 38,\n",
       "         'many': 2,\n",
       "         'things': 14,\n",
       "         'and': 80,\n",
       "         'people': 6,\n",
       "         'around': 5,\n",
       "         'that': 101,\n",
       "         'makes': 2,\n",
       "         'sense': 3,\n",
       "         'honestly': 10,\n",
       "         'just': 39,\n",
       "         'looking': 2,\n",
       "         'for': 35,\n",
       "         'something': 8,\n",
       "         'pass': 1,\n",
       "         'the': 91,\n",
       "         'time': 21,\n",
       "         'like': 34,\n",
       "         'well': 25,\n",
       "         'there': 11,\n",
       "         'bicycle': 1,\n",
       "         'wanted': 15,\n",
       "         'more': 13,\n",
       "         'than': 6,\n",
       "         'anything': 5,\n",
       "         'when': 28,\n",
       "         'remember': 29,\n",
       "         'this': 28,\n",
       "         'story': 3,\n",
       "         'vaguely': 1,\n",
       "         'happened': 1,\n",
       "         'got': 21,\n",
       "         'science': 1,\n",
       "         'kit': 1,\n",
       "         'my': 42,\n",
       "         'birthday': 2,\n",
       "         'crappy': 1,\n",
       "         'ones': 1,\n",
       "         'can': 6,\n",
       "         'buy': 2,\n",
       "         'toy': 2,\n",
       "         'shop': 1,\n",
       "         'yes': 30,\n",
       "         'but': 55,\n",
       "         'did': 20,\n",
       "         'collected': 1,\n",
       "         'same': 5,\n",
       "         'chemical': 1,\n",
       "         'over': 5,\n",
       "         'again': 2,\n",
       "         'from': 13,\n",
       "         'pharmacies': 1,\n",
       "         'enough': 1,\n",
       "         'make': 6,\n",
       "         'explosion': 1,\n",
       "         'enormous': 1,\n",
       "         '…': 1,\n",
       "         'blew': 1,\n",
       "         'roof': 2,\n",
       "         'off': 11,\n",
       "         'house': 5,\n",
       "         'hole': 2,\n",
       "         '-': 4,\n",
       "         'parents': 3,\n",
       "         'were': 28,\n",
       "         'poor': 3,\n",
       "         'we': 24,\n",
       "         'lived': 1,\n",
       "         'with': 29,\n",
       "         'our': 6,\n",
       "         'months': 2,\n",
       "         'say': 12,\n",
       "         'they': 9,\n",
       "         'both': 6,\n",
       "         'angry': 3,\n",
       "         'had': 18,\n",
       "         'lot': 7,\n",
       "         'love': 5,\n",
       "         'me': 43,\n",
       "         'fine': 9,\n",
       "         'end': 1,\n",
       "         'winter': 1,\n",
       "         'thankfully': 1,\n",
       "         'not': 33,\n",
       "         'quite': 5,\n",
       "         'cold': 1,\n",
       "         'though': 11,\n",
       "         'at': 23,\n",
       "         'times': 4,\n",
       "         'do': 44,\n",
       "         'back': 6,\n",
       "         'those': 9,\n",
       "         'very': 5,\n",
       "         'fondly': 1,\n",
       "         'however': 2,\n",
       "         'life': 7,\n",
       "         'simple': 2,\n",
       "         'maybe': 4,\n",
       "         'that’s': 24,\n",
       "         'how': 14,\n",
       "         'no': 18,\n",
       "         'yep': 3,\n",
       "         'everything': 9,\n",
       "         'being': 4,\n",
       "         'much': 10,\n",
       "         'easier': 2,\n",
       "         'wake': 2,\n",
       "         'watch': 2,\n",
       "         'cartoons': 2,\n",
       "         'on': 27,\n",
       "         'saturday': 2,\n",
       "         'pancakes': 1,\n",
       "         'smell': 1,\n",
       "         'most': 7,\n",
       "         'feeling': 1,\n",
       "         'warm': 2,\n",
       "         'sun': 1,\n",
       "         'coming': 1,\n",
       "         'through': 4,\n",
       "         'windows': 1,\n",
       "         'all': 18,\n",
       "         'plants': 2,\n",
       "         'have': 28,\n",
       "         'course': 2,\n",
       "         'important': 1,\n",
       "         'grow': 2,\n",
       "         'really': 38,\n",
       "         'why': 8,\n",
       "         'i’m': 31,\n",
       "         'entirely': 1,\n",
       "         'sure': 13,\n",
       "         'because': 15,\n",
       "         'younger': 7,\n",
       "         'struggle': 1,\n",
       "         'accept': 2,\n",
       "         'big': 3,\n",
       "         'disposal': 1,\n",
       "         'including': 1,\n",
       "         'middle': 1,\n",
       "         'class': 1,\n",
       "         'family': 2,\n",
       "         'nothing': 3,\n",
       "         'handed': 1,\n",
       "         'feel': 6,\n",
       "         'such': 6,\n",
       "         'an': 7,\n",
       "         'integral': 1,\n",
       "         'part': 5,\n",
       "         'education': 1,\n",
       "         'fend': 1,\n",
       "         'myself': 3,\n",
       "         'assumed': 1,\n",
       "         'learn': 1,\n",
       "         'stuff': 10,\n",
       "         'too': 15,\n",
       "         'young': 5,\n",
       "         'age': 4,\n",
       "         'mean': 27,\n",
       "         'it’s': 20,\n",
       "         'force': 1,\n",
       "         'go': 9,\n",
       "         'out': 6,\n",
       "         'shops': 2,\n",
       "         'dark': 1,\n",
       "         'or': 14,\n",
       "         'getting': 6,\n",
       "         'motorbike': 1,\n",
       "         'scary': 1,\n",
       "         'yeah': 26,\n",
       "         'best': 3,\n",
       "         'move': 1,\n",
       "         'be': 30,\n",
       "         '‘streetwise’': 1,\n",
       "         'ten': 2,\n",
       "         'years': 2,\n",
       "         'old': 3,\n",
       "         'would': 24,\n",
       "         'come': 5,\n",
       "         'naturally': 1,\n",
       "         'sorry…': 1,\n",
       "         'these': 1,\n",
       "         'didn’t': 18,\n",
       "         'sometimes': 6,\n",
       "         'ok': 8,\n",
       "         'now': 21,\n",
       "         'glad': 3,\n",
       "         'father': 3,\n",
       "         'loved': 3,\n",
       "         'him': 12,\n",
       "         'difficult': 6,\n",
       "         'said': 8,\n",
       "         'tried': 2,\n",
       "         'better': 6,\n",
       "         'holidays': 2,\n",
       "         'food': 1,\n",
       "         'etc…': 1,\n",
       "         'doesn’t': 2,\n",
       "         'less': 1,\n",
       "         'surely': 1,\n",
       "         'oh': 18,\n",
       "         'completely': 2,\n",
       "         'even': 9,\n",
       "         'understand': 3,\n",
       "         'effort': 1,\n",
       "         'he': 22,\n",
       "         'put': 3,\n",
       "         'into': 10,\n",
       "         'making': 1,\n",
       "         'communist': 2,\n",
       "         'wasn’t': 8,\n",
       "         'party': 1,\n",
       "         'while': 4,\n",
       "         'grew': 1,\n",
       "         'socialist': 1,\n",
       "         'giving': 2,\n",
       "         'if': 14,\n",
       "         'needed': 4,\n",
       "         'lacking': 1,\n",
       "         'today': 1,\n",
       "         'grandpa': 5,\n",
       "         'bring': 2,\n",
       "         'home': 3,\n",
       "         'homeless': 2,\n",
       "         'drive': 1,\n",
       "         'grandma': 3,\n",
       "         'insane': 1,\n",
       "         'honest': 2,\n",
       "         'caring': 1,\n",
       "         'am': 4,\n",
       "         'others': 1,\n",
       "         'trustworthy': 1,\n",
       "         'grandma’s': 1,\n",
       "         'side': 2,\n",
       "         'incident…': 1,\n",
       "         'incident': 1,\n",
       "         'basically': 4,\n",
       "         'brought': 1,\n",
       "         'guy': 3,\n",
       "         'sleep': 2,\n",
       "         'bare': 2,\n",
       "         'mind': 2,\n",
       "         'space': 1,\n",
       "         'uncle': 1,\n",
       "         'yeah…': 1,\n",
       "         'anyways': 5,\n",
       "         'gone': 1,\n",
       "         'unusual': 1,\n",
       "         'he’s': 1,\n",
       "         'spent': 1,\n",
       "         'night': 3,\n",
       "         'right': 7,\n",
       "         'bathroom': 2,\n",
       "         'literally': 6,\n",
       "         'smeared': 1,\n",
       "         'shit': 8,\n",
       "         'walls': 1,\n",
       "         'clean': 4,\n",
       "         'up…': 1,\n",
       "         'man…': 1,\n",
       "         'rough': 1,\n",
       "         'courage': 2,\n",
       "         'trust': 3,\n",
       "         'wrath': 1,\n",
       "         'sad': 1,\n",
       "         'knew': 2,\n",
       "         'yet': 1,\n",
       "         'still': 4,\n",
       "         'miss': 1,\n",
       "         'god': 3,\n",
       "         'sister': 4,\n",
       "         'few': 1,\n",
       "         'clear': 1,\n",
       "         'memories': 1,\n",
       "         'chateau': 1,\n",
       "         'lignan': 1,\n",
       "         'where': 4,\n",
       "         'holiday': 2,\n",
       "         'south': 1,\n",
       "         'france': 1,\n",
       "         'some': 9,\n",
       "         'visit': 1,\n",
       "         'us': 1,\n",
       "         'lola': 2,\n",
       "         'presents': 1,\n",
       "         'snotty': 1,\n",
       "         'stupid': 1,\n",
       "         'appreciating': 1,\n",
       "         'his': 2,\n",
       "         'gifts': 2,\n",
       "         'smile': 1,\n",
       "         'thank': 2,\n",
       "         'nice': 1,\n",
       "         'inside': 1,\n",
       "         'thinking': 2,\n",
       "         'colouring': 1,\n",
       "         'books': 1,\n",
       "         'pencils': 1,\n",
       "         'etc': 3,\n",
       "         'touch': 3,\n",
       "         'guys': 2,\n",
       "         'money': 2,\n",
       "         'spend': 1,\n",
       "         'cried': 2,\n",
       "         'see': 5,\n",
       "         'beautiful': 2,\n",
       "         'gesture': 1,\n",
       "         'amazing': 3,\n",
       "         'man': 8,\n",
       "         'was…': 1,\n",
       "         'wear': 3,\n",
       "         'gold': 3,\n",
       "         'chain': 3,\n",
       "         'gifted': 3,\n",
       "         'once': 2,\n",
       "         'wait': 5,\n",
       "         'actually': 3,\n",
       "         'subject': 1,\n",
       "         'enjoyed': 1,\n",
       "         'then': 10,\n",
       "         'length': 1,\n",
       "         'other': 1,\n",
       "         \"wouldn't\": 3,\n",
       "         'whereas': 1,\n",
       "         'grandmother': 1,\n",
       "         \"it's\": 3,\n",
       "         'she': 10,\n",
       "         'likes': 2,\n",
       "         \"don't\": 6,\n",
       "         \"she'd\": 1,\n",
       "         'idea': 2,\n",
       "         'kind': 3,\n",
       "         'wish': 2,\n",
       "         'fault': 2,\n",
       "         'thin': 1,\n",
       "         'guitar': 5,\n",
       "         'pendant': 2,\n",
       "         'recently': 1,\n",
       "         'found': 2,\n",
       "         'year': 6,\n",
       "         'ago': 4,\n",
       "         'take': 2,\n",
       "         'seeing': 1,\n",
       "         'neck': 2,\n",
       "         'taking': 1,\n",
       "         'piss': 3,\n",
       "         'look': 8,\n",
       "         'gangster': 1,\n",
       "         'grandpas': 1,\n",
       "         'took': 4,\n",
       "         'comes': 2,\n",
       "         'sweet': 1,\n",
       "         'bet': 2,\n",
       "         'happy': 5,\n",
       "         'hear': 7,\n",
       "         'hope': 1,\n",
       "         'regret': 3,\n",
       "         'bought': 2,\n",
       "         'new': 6,\n",
       "         'dog': 1,\n",
       "         'certainly': 1,\n",
       "         'emotional': 1,\n",
       "         'respected': 1,\n",
       "         'older': 2,\n",
       "         'respect': 1,\n",
       "         'tap': 1,\n",
       "         'their': 3,\n",
       "         'emotions': 1,\n",
       "         'we’re': 5,\n",
       "         'talking—': 1,\n",
       "         'minute': 1,\n",
       "         'okay': 10,\n",
       "         'son': 7,\n",
       "         'please': 2,\n",
       "         'does': 4,\n",
       "         'moment': 1,\n",
       "         'i…': 1,\n",
       "         'nope': 1,\n",
       "         'can’t': 4,\n",
       "         'wait…what': 1,\n",
       "         'like…ten': 1,\n",
       "         'sat': 1,\n",
       "         'down': 4,\n",
       "         'seven': 1,\n",
       "         'math': 1,\n",
       "         'tutor': 1,\n",
       "         'worth': 2,\n",
       "         '“ahem': 1,\n",
       "         'uh…listen': 1,\n",
       "         'want': 9,\n",
       "         'differently…if': 1,\n",
       "         'ever': 6,\n",
       "         'begin': 2,\n",
       "         '‘because': 1,\n",
       "         'so’': 1,\n",
       "         'gotta': 3,\n",
       "         'slap': 1,\n",
       "         'remind': 1,\n",
       "         'hated': 2,\n",
       "         'hearing': 1,\n",
       "         'up”': 1,\n",
       "         'voice…that': 1,\n",
       "         'supposed': 1,\n",
       "         'sound': 1,\n",
       "         'seriously': 1,\n",
       "         'serious': 1,\n",
       "         'need': 2,\n",
       "         'it…now': 1,\n",
       "         'soon': 2,\n",
       "         'mother': 5,\n",
       "         'will': 2,\n",
       "         'here': 5,\n",
       "         'any': 8,\n",
       "         'problem': 1,\n",
       "         'because…': 1,\n",
       "         'c’mon': 1,\n",
       "         'you’re': 4,\n",
       "         'almost': 2,\n",
       "         'grown': 2,\n",
       "         'start': 1,\n",
       "         'taking…': 1,\n",
       "         'care': 1,\n",
       "         'mom': 2,\n",
       "         'are': 6,\n",
       "         'ready': 1,\n",
       "         'already': 2,\n",
       "         'finished': 1,\n",
       "         'coll–': 1,\n",
       "         'save': 1,\n",
       "         'whoa': 1,\n",
       "         'what’s': 6,\n",
       "         'i’ll': 6,\n",
       "         'mean…jesus': 1,\n",
       "         'matter': 1,\n",
       "         'guess': 8,\n",
       "         'you’ll…': 1,\n",
       "         'shit…lemme': 1,\n",
       "         'uhwasn’t': 1,\n",
       "         'going': 8,\n",
       "         'just…please': 1,\n",
       "         'she’ll': 1,\n",
       "         'afraid': 2,\n",
       "         'her': 12,\n",
       "         '‘have': 1,\n",
       "         'her’': 1,\n",
       "         'fuck': 2,\n",
       "         'nerve': 1,\n",
       "         'to…': 1,\n",
       "         'college': 1,\n",
       "         'changing': 1,\n",
       "         'ways': 1,\n",
       "         'expect': 1,\n",
       "         'i’': 1,\n",
       "         'm': 1,\n",
       "         'sorry': 8,\n",
       "         'was…out': 1,\n",
       "         'line': 1,\n",
       "         'i’m…sorry': 1,\n",
       "         'fo': 1,\n",
       "         'deserved': 1,\n",
       "         'defnitely': 1,\n",
       "         'i’d…actually': 1,\n",
       "         'disagree': 1,\n",
       "         'made': 1,\n",
       "         'who': 4,\n",
       "         'dads': 2,\n",
       "         'top': 1,\n",
       "         'parenting': 1,\n",
       "         'resume': 1,\n",
       "         'deal': 2,\n",
       "         'spanked': 1,\n",
       "         'kids': 3,\n",
       "         'acted': 1,\n",
       "         'meant': 1,\n",
       "         'well…mine': 1,\n",
       "         'uh-huh': 1,\n",
       "         'here’s': 1,\n",
       "         'thing': 9,\n",
       "         'test': 3,\n",
       "         'two': 2,\n",
       "         'seconds': 1,\n",
       "         'least': 2,\n",
       "         'they’re': 1,\n",
       "         'certain': 4,\n",
       "         'set': 1,\n",
       "         '‘em': 1,\n",
       "         'straight': 1,\n",
       "         'sit': 2,\n",
       "         'bookshelf': 1,\n",
       "         'lemme': 3,\n",
       "         'just…here': 1,\n",
       "         'ya': 1,\n",
       "         'thanks': 1,\n",
       "         'huh': 3,\n",
       "         'comfy': 1,\n",
       "         'woulda': 1,\n",
       "         'imagined': 1,\n",
       "         'worn': 1,\n",
       "         'past': 2,\n",
       "         'roommate': 1,\n",
       "         'i’d': 2,\n",
       "         'god’s': 1,\n",
       "         'sake…i': 1,\n",
       "         'them': 7,\n",
       "         'couple': 1,\n",
       "         'nights': 1,\n",
       "         'before': 1,\n",
       "         'went': 1,\n",
       "         'must': 2,\n",
       "         'next': 2,\n",
       "         'bed': 2,\n",
       "         'um…': 1,\n",
       "         'chair': 2,\n",
       "         'asked': 2,\n",
       "         'coffee': 1,\n",
       "         'that’d': 1,\n",
       "         'nice…except': 1,\n",
       "         'came': 1,\n",
       "         'first': 6,\n",
       "         'place': 3,\n",
       "         'willing': 1,\n",
       "         'remarkable': 1,\n",
       "         'ha': 2,\n",
       "         'guilty': 1,\n",
       "         'know…your': 1,\n",
       "         'whatever': 2,\n",
       "         'rush': 1,\n",
       "         'kiddin’': 1,\n",
       "         'gettin’': 1,\n",
       "         'let': 3,\n",
       "         'ugh…she': 1,\n",
       "         'takes': 1,\n",
       "         'wrong': 2,\n",
       "         'son…she’s': 1,\n",
       "         'cares': 1,\n",
       "         'only': 4,\n",
       "         'tells': 1,\n",
       "         'suggesting': 1,\n",
       "         'open': 1,\n",
       "         'dick': 1,\n",
       "         'gonna': 1,\n",
       "         'prefer': 1,\n",
       "         'bed…for': 1,\n",
       "         'obvious': 1,\n",
       "         'reasons': 1,\n",
       "         'meantime': 1,\n",
       "         'exhibit': 1,\n",
       "         'heard': 1,\n",
       "         'earlier': 1,\n",
       "         'okay…dad': 1,\n",
       "         'why’d': 1,\n",
       "         'happen': 1,\n",
       "         'forgot': 2,\n",
       "         'couldn’t': 3,\n",
       "         'finish': 2,\n",
       "         'last': 2,\n",
       "         'she’s': 2,\n",
       "         'second': 2,\n",
       "         'we’ll': 1,\n",
       "         'sopping': 1,\n",
       "         'beer': 2,\n",
       "         'carpet': 2,\n",
       "         'dammit': 1,\n",
       "         'imports': 1,\n",
       "         'cleaning': 2,\n",
       "         'liked': 3,\n",
       "         'fancy': 1,\n",
       "         'beers': 1,\n",
       "         'splurge': 1,\n",
       "         'thought': 2,\n",
       "         'drink': 1,\n",
       "         'forget': 1,\n",
       "         'nineteen': 1,\n",
       "         'impossible': 1,\n",
       "         'room’s': 1,\n",
       "         'there’s': 1,\n",
       "         'triple-pissed': 1,\n",
       "         'fun': 3,\n",
       "         'weekend': 1,\n",
       "         'relax': 1,\n",
       "         'shut': 1,\n",
       "         'crouched': 1,\n",
       "         'fucking': 2,\n",
       "         'floor': 5,\n",
       "         'goddammit': 1,\n",
       "         'tense': 1,\n",
       "         'here…sort': 1,\n",
       "         'fall': 1,\n",
       "         'beanbag': 2,\n",
       "         'you…': 2,\n",
       "         'rubbed': 1,\n",
       "         'against': 2,\n",
       "         'she’d': 1,\n",
       "         'saying': 2,\n",
       "         'torture': 1,\n",
       "         'any…pills': 1,\n",
       "         'my…i': 1,\n",
       "         'mean…wait': 1,\n",
       "         'uhput': 1,\n",
       "         'them…': 1,\n",
       "         'yelling': 1,\n",
       "         'isn’t': 1,\n",
       "         'speed': 1,\n",
       "         'memory': 1,\n",
       "         'seen': 1,\n",
       "         'nightstand': 1,\n",
       "         'just…breathe…': 1,\n",
       "         'baby': 1,\n",
       "         'mean…relax': 1,\n",
       "         'check': 1,\n",
       "         'aren’t': 1,\n",
       "         'find': 2,\n",
       "         'jesus': 1,\n",
       "         'either': 1,\n",
       "         'gimme': 1,\n",
       "         'water': 1,\n",
       "         'bottle’s': 1,\n",
       "         'empty': 2,\n",
       "         'bottle': 1,\n",
       "         'filled': 2,\n",
       "         'it…lemme': 1,\n",
       "         'see…when': 1,\n",
       "         'go…': 1,\n",
       "         'says': 1,\n",
       "         'refill': 1,\n",
       "         'allowed': 2,\n",
       "         'doctor’s': 1,\n",
       "         'approval': 1,\n",
       "         'until': 2,\n",
       "         'monday': 2,\n",
       "         'lay': 1,\n",
       "         'face': 1,\n",
       "         'in…ugh…whatever’s': 1,\n",
       "         'fucked': 2,\n",
       "         'kidding': 1,\n",
       "         'door': 1,\n",
       "         \"i'm\": 3,\n",
       "         'law': 2,\n",
       "         'school': 4,\n",
       "         'called': 1,\n",
       "         'congratulations': 1,\n",
       "         \"you'll\": 1,\n",
       "         'supreme': 1,\n",
       "         'court': 1,\n",
       "         'may': 2,\n",
       "         'chase': 1,\n",
       "         'ambulances': 1,\n",
       "         \"how's\": 1,\n",
       "         'raymond': 1,\n",
       "         \"he's\": 1,\n",
       "         'ocs': 1,\n",
       "         'army': 1,\n",
       "         \"that's\": 4,\n",
       "         'bad': 3,\n",
       "         \"haven't\": 1,\n",
       "         'jennifer': 1,\n",
       "         \"she's\": 5,\n",
       "         'absolutely': 1,\n",
       "         'charming': 1,\n",
       "         'background': 1,\n",
       "         'radcliffe': 2,\n",
       "         'point': 2,\n",
       "         \"doesn't\": 1,\n",
       "         'concern': 1,\n",
       "         'rebellion': 2,\n",
       "         'rebelling': 2,\n",
       "         'fail': 2,\n",
       "         'marrying': 1,\n",
       "         'brilliant': 1,\n",
       "         'girl': 1,\n",
       "         'constitutes': 1,\n",
       "         'crazy': 2,\n",
       "         'hippie': 1,\n",
       "         'irks': 1,\n",
       "         'catholic': 1,\n",
       "         'attracts': 1,\n",
       "         'leaving': 2,\n",
       "         'half-cocked': 1,\n",
       "         'ask': 3,\n",
       "         'define': 1,\n",
       "         '\"bit\"': 1,\n",
       "         'real': 2,\n",
       "         \"it'll\": 1,\n",
       "         'stand': 1,\n",
       "         'should': 1,\n",
       "         'asking': 1,\n",
       "         \"you're\": 1,\n",
       "         'commanding': 1,\n",
       "         'marry': 1,\n",
       "         \"i'll\": 1,\n",
       "         'give': 1,\n",
       "         'day': 5,\n",
       "         'know…': 2,\n",
       "         'saving': 1,\n",
       "         'cycling': 1,\n",
       "         'sport': 1,\n",
       "         'interested': 1,\n",
       "         'bored': 3,\n",
       "         'eventually': 2,\n",
       "         'stay': 1,\n",
       "         'selfish': 1,\n",
       "         'known': 2,\n",
       "         'born': 1,\n",
       "         'doing': 2,\n",
       "         'extent': 1,\n",
       "         'harder': 1,\n",
       "         'especially': 2,\n",
       "         'after': 2,\n",
       "         'divorce': 1,\n",
       "         'really…': 1,\n",
       "         'changed': 1,\n",
       "         'became': 1,\n",
       "         'felt': 6,\n",
       "         'suddenly': 1,\n",
       "         'fair': 2,\n",
       "         'talked': 1,\n",
       "         'mum': 7,\n",
       "         'happening': 1,\n",
       "         'react': 1,\n",
       "         'do…': 1,\n",
       "         'creating': 1,\n",
       "         'sides': 1,\n",
       "         'pick': 2,\n",
       "         'tell': 5,\n",
       "         'which': 2,\n",
       "         'clearly': 4,\n",
       "         'untrue': 1,\n",
       "         'head': 3,\n",
       "         'smoked': 1,\n",
       "         'cigarettes': 1,\n",
       "         'impressionable': 1,\n",
       "         'saw': 1,\n",
       "         'horrible': 1,\n",
       "         'stressed': 1,\n",
       "         'die': 1,\n",
       "         'brining': 1,\n",
       "         'conversation': 3,\n",
       "         'holding': 1,\n",
       "         'grudges': 1,\n",
       "         'deep': 1,\n",
       "         'subconscious': 1,\n",
       "         'badly': 2,\n",
       "         'thankful': 1,\n",
       "         'arguments': 1,\n",
       "         '16': 1,\n",
       "         'damn': 7,\n",
       "         'along': 1,\n",
       "         'experience': 1,\n",
       "         'raising': 1,\n",
       "         'teenager': 1,\n",
       "         'thicker': 1,\n",
       "         'skin': 1,\n",
       "         'also': 3,\n",
       "         'clashing': 2,\n",
       "         'personalities': 1,\n",
       "         'similar': 2,\n",
       "         'obsessive': 3,\n",
       "         'nature': 2,\n",
       "         'obsess': 1,\n",
       "         'needs': 1,\n",
       "         'internally': 1,\n",
       "         'freak': 1,\n",
       "         'slight': 1,\n",
       "         'ocd': 2,\n",
       "         'twice': 1,\n",
       "         'hoovering': 1,\n",
       "         '8am': 1,\n",
       "         'compulsive': 1,\n",
       "         'keeping': 1,\n",
       "         'commend': 1,\n",
       "         'taught': 2,\n",
       "         'bother': 1,\n",
       "         'example': 2,\n",
       "         'you’d': 2,\n",
       "         'nuts': 1,\n",
       "         'walked': 1,\n",
       "         'feet': 1,\n",
       "         'slippers': 1,\n",
       "         'everywhere': 1,\n",
       "         'special': 1,\n",
       "         'white': 1,\n",
       "         'resin': 1,\n",
       "         'dirty': 1,\n",
       "         'super': 1,\n",
       "         'fast': 1,\n",
       "         'consisted': 1,\n",
       "         'listening': 1,\n",
       "         'tail': 1,\n",
       "         'between': 2,\n",
       "         'legs': 1,\n",
       "         'hit': 1,\n",
       "         'sixteen': 2,\n",
       "         'own': 1,\n",
       "         'clashed': 1,\n",
       "         'anymore': 2,\n",
       "         'silly': 1,\n",
       "         'started': 3,\n",
       "         'general…': 1,\n",
       "         'vegetables': 3,\n",
       "         'lucas': 1,\n",
       "         'on…': 1,\n",
       "         'obsession': 1,\n",
       "         'bothered': 1,\n",
       "         'health': 2,\n",
       "         'ate': 2,\n",
       "         'plate': 1,\n",
       "         'salad': 2,\n",
       "         'tomatoes': 2,\n",
       "         'eat': 2,\n",
       "         'could': 3,\n",
       "         'complain': 1,\n",
       "         'petty': 1,\n",
       "         'scared': 1,\n",
       "         'mean…': 1,\n",
       "         'otherwise': 1,\n",
       "         'pissed': 1,\n",
       "         'off…': 1,\n",
       "         'phones': 3,\n",
       "         'obsessed': 2,\n",
       "         'constantly': 2,\n",
       "         'phone': 3,\n",
       "         'again…': 1,\n",
       "         'weren’t': 1,\n",
       "         'decent': 1,\n",
       "         'amount': 1,\n",
       "         'ipod': 3,\n",
       "         'everyone': 1,\n",
       "         'definitely': 2,\n",
       "         'diego': 1,\n",
       "         'jealous…': 1,\n",
       "         'christmas': 5,\n",
       "         'gift': 1,\n",
       "         'playing': 2,\n",
       "         'hours': 1,\n",
       "         'spending': 1,\n",
       "         'devices': 1,\n",
       "         'whole': 2,\n",
       "         'world': 1,\n",
       "         'able': 1,\n",
       "         'download': 1,\n",
       "         'games': 1,\n",
       "         'understood': 2,\n",
       "         'gaming': 2,\n",
       "         'attractive': 1,\n",
       "         'small': 1,\n",
       "         'screen': 1,\n",
       "         'console': 2,\n",
       "         'true': 5,\n",
       "         'nintendo': 2,\n",
       "         'ds': 2,\n",
       "         'life…': 1,\n",
       "         'tiger': 1,\n",
       "         'imagining': 1,\n",
       "         'try': 2,\n",
       "         'cute': 1,\n",
       "         'wanting': 3,\n",
       "         'pre-naughties': 1,\n",
       "         'present': 1,\n",
       "         'asks': 1,\n",
       "         'bicycles': 1,\n",
       "         'bike': 1,\n",
       "         'mario': 1,\n",
       "         'kart': 1,\n",
       "         'trying': 2,\n",
       "         'exciting': 1,\n",
       "         'interest': 1,\n",
       "         'died': 1,\n",
       "         'kept': 1,\n",
       "         'pestering': 1,\n",
       "         'every': 4,\n",
       "         'single': 2,\n",
       "         'text': 4,\n",
       "         'annoyed': 1,\n",
       "         'reason': 1,\n",
       "         'overbearing': 1,\n",
       "         'technology': 1,\n",
       "         'tiny': 1,\n",
       "         'told': 3,\n",
       "         'surprise': 1,\n",
       "         'bus': 1,\n",
       "         'shopping': 1,\n",
       "         'centre': 1,\n",
       "         'starting': 1,\n",
       "         'music': 3,\n",
       "         'cool': 1,\n",
       "         'collect': 1,\n",
       "         'angsty': 2,\n",
       "         'rock': 3,\n",
       "         'stage': 1,\n",
       "         'black': 2,\n",
       "         'skinny': 3,\n",
       "         'jeans': 2,\n",
       "         'phase': 1,\n",
       "         'else': 1,\n",
       "         'truly': 1,\n",
       "         'believed': 1,\n",
       "         'jeans…': 1,\n",
       "         'goodness': 1,\n",
       "         'grunge': 1,\n",
       "         'truthfully': 1,\n",
       "         'goes': 1,\n",
       "         'yours': 1,\n",
       "         'brother': 1,\n",
       "         '‘group’': 1,\n",
       "         'belong': 2,\n",
       "         'group': 1,\n",
       "         'friends': 2,\n",
       "         'pj': 1,\n",
       "         'max': 1,\n",
       "         'played': 1,\n",
       "         'sounds': 1,\n",
       "         'right…': 1,\n",
       "         'electric': 1,\n",
       "         'lesson': 1,\n",
       "         'shame': 1,\n",
       "         'cello': 1,\n",
       "         'plus': 1,\n",
       "         'forced': 1,\n",
       "         'play': 1,\n",
       "         'appreciate': 1,\n",
       "         'skateboarding': 1,\n",
       "         'talking': 1,\n",
       "         'tricks': 1,\n",
       "         'lessons': 1,\n",
       "         'hundreds': 1,\n",
       "         'ended': 1,\n",
       "         'of…': 1,\n",
       "         'computer': 3,\n",
       "         'geeky': 1,\n",
       "         'opposite': 1,\n",
       "         'proud': 2,\n",
       "         'help': 1,\n",
       "         'self': 1,\n",
       "         'generation': 1,\n",
       "         'use': 1,\n",
       "         'correctly': 1,\n",
       "         'period': 1,\n",
       "         'normally': 1,\n",
       "         'suppose': 3,\n",
       "         'experiences…': 1,\n",
       "         'spoke': 1,\n",
       "         'surprised': 1,\n",
       "         'opinion': 1,\n",
       "         'hard': 1,\n",
       "         'interesting': 1,\n",
       "         'converse': 1,\n",
       "         'degree': 1,\n",
       "         'interaction': 1,\n",
       "         'view': 1,\n",
       "         'differences': 1,\n",
       "         'men': 1,\n",
       "         'women': 1,\n",
       "         'shout': 1,\n",
       "         'choose': 2,\n",
       "         'ignore': 1,\n",
       "         'worse': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in range(numIterations):\n",
    "\n",
    "    encoderTrain, decoderTargetTrain, decoderInputTrain = getTrainingBatch(xTrain, yTrain, batchSize, maxEncoderLength)\n",
    "    feedDict = {encoderInputs[t]: encoderTrain[t] for t in range(maxEncoderLength)}\n",
    "    feedDict.update({decoderLabels[t]: decoderTargetTrain[t] for t in range(maxDecoderLength)})\n",
    "    feedDict.update({decoderInputs[t]: decoderInputTrain[t] for t in range(maxDecoderLength)})\n",
    "    feedDict.update({feedPrevious: False})\n",
    "\n",
    "    curLoss, _, pred = sess.run([loss, optimizer, decoderPrediction], feed_dict=feedDict)\n",
    "\n",
    "    if (i % 200 == 0):\n",
    "        print('Current loss:', curLoss, 'at iteration', i)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        summary = sess.run(merged, feed_dict=feedDict)\n",
    "        writer.add_summary(summary, i)\n",
    "    if (i % 100 == 0 and i != 0):\n",
    "        num = randint(0,len(encoderTestStrings) - 1)\n",
    "        print(encoderTestStrings[num])\n",
    "        inputVector = getTestInput(encoderTestStrings[num], wordList, maxEncoderLength);\n",
    "        feedDict = {encoderInputs[t]: inputVector[t] for t in range(maxEncoderLength)}\n",
    "        feedDict.update({decoderLabels[t]: zeroVector for t in range(maxDecoderLength)})\n",
    "        feedDict.update({decoderInputs[t]: zeroVector for t in range(maxDecoderLength)})\n",
    "        feedDict.update({feedPrevious: True})\n",
    "        ids = (sess.run(decoderPrediction, feed_dict=feedDict))\n",
    "        print(idsToSentence(ids, wordList))\n",
    "\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        savePath = saver.save(sess, \"models/son/sonpretrained_seq2seq.ckpt\", global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
