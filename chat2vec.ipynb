{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import pprint\n",
    "\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "import sklearn.manifold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files:  ['files/001.txt', 'files/002.txt', 'files/003.txt', 'files/004.txt', 'files/005.txt', 'files/006.txt', 'files/007.txt', 'files/008.txt']\n"
     ]
    }
   ],
   "source": [
    "#Read files\n",
    "conv_filename = sorted(glob.glob('files/*.txt'))\n",
    "print(\"Found files: \", conv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'files/001.txt'...\n",
      "Corpus is now 7284 characters long\n",
      "\n",
      "Reading 'files/002.txt'...\n",
      "Corpus is now 14068 characters long\n",
      "\n",
      "Reading 'files/003.txt'...\n",
      "Corpus is now 15411 characters long\n",
      "\n",
      "Reading 'files/004.txt'...\n",
      "Corpus is now 26549 characters long\n",
      "\n",
      "Reading 'files/005.txt'...\n",
      "Corpus is now 28596 characters long\n",
      "\n",
      "Reading 'files/006.txt'...\n",
      "Corpus is now 30494 characters long\n",
      "\n",
      "Reading 'files/007.txt'...\n",
      "Corpus is now 39712 characters long\n",
      "\n",
      "Reading 'files/008.txt'...\n",
      "Corpus is now 48616 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine all files in corpus\n",
    "corpus_raw = u\"\"\n",
    "for conv_filename in conv_filename:\n",
    "    print(\"Reading '{0}'...\".format(conv_filename))\n",
    "    corpus_raw += ' *----------* '\n",
    "    with codecs.open(conv_filename, \"r\", \"utf-8\") as conv_file:\n",
    "        corpus_raw += conv_file.read()\n",
    "\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn words into tokens\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = re.sub(r'\\n+','.', corpus_raw)\n",
    "corpus_raw = re.sub(r'\\.+', \". \", corpus_raw)\n",
    "corpus_raw = re.sub(r'…', \" … \", corpus_raw)\n",
    "corpus_raw = re.sub(r'’', \"\", corpus_raw)\n",
    "corpus_raw = re.sub(r\"/'\", \"\", corpus_raw)\n",
    "corpus_raw = re.sub(r' +', \" \", corpus_raw)\n",
    "corpus_raw = re.sub(r'son:','', corpus_raw)\n",
    "corpus_raw = re.sub(r'father:','', corpus_raw)\n",
    "corpus_raw = re.sub(\" +\",\" \", corpus_raw)\n",
    "\n",
    "#convert to everything to lowercase\n",
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#remove unnnecessary,, split into words, no hyphens\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"/'\",\"\", raw)\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing and cleaning dataset\n",
      "1324 unique words found\n",
      "Created file data/wordList.txt\n"
     ]
    }
   ],
   "source": [
    "def processDataset(allLines):\n",
    "    myStr = \"\"\n",
    "    for line in allLines:\n",
    "        myStr += line\n",
    "    myStr = re.sub(\"[^a-zA-Z]\",\" \", myStr)\n",
    "    finalDict = Counter(myStr.split())\n",
    "    return myStr, finalDict\n",
    "\n",
    "fullCorpus, datasetDictionary = processDataset(corpus_raw)\n",
    "print('Finished parsing and cleaning dataset')\n",
    "wordList = list(datasetDictionary.keys())\n",
    "\n",
    "with open(\"data/wordList.txt\", \"wb\") as fp:\n",
    "\tpickle.dump(wordList, fp)\n",
    "print(len(wordList), 'unique words found')\n",
    "print('Created file data/wordList.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chat corpus contains 8,654 tokens\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)\n",
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "\n",
    "# print(raw_sentences[890])\n",
    "# print(sentence_to_wordlist(raw_sentences[890]))\n",
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The chat corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 435\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 0\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1\n",
    "\n",
    "chat2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "chat2vec.build_vocab(sentences)\n",
    "\n",
    "print(\"Word2Vec vocabulary length:\", len(chat2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75460, 86540)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat2vec.train(sentences, total_examples=chat2vec.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/embeddingMatrix.npy', chat2vec.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
