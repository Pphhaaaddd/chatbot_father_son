{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import pprint\n",
    "\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanMessage(corpus_raw):\n",
    "    corpus_raw = re.sub(r'\\n+','.', corpus_raw)\n",
    "    corpus_raw = re.sub(r'\\.+', \". \", corpus_raw)\n",
    "    corpus_raw = re.sub(r'…', \" … \", corpus_raw)\n",
    "    corpus_raw = re.sub(r'’', \"\", corpus_raw)\n",
    "    corpus_raw = corpus_raw.replace(\"'\", \"\")\n",
    "    corpus_raw = re.sub(r' +', \" \", corpus_raw)\n",
    "    corpus_raw = re.sub(\" +\",\" \", corpus_raw)\n",
    "    return corpus_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files:  ['files/001.txt', 'files/002.txt', 'files/003.txt', 'files/004.txt', 'files/005.txt', 'files/006.txt', 'files/007.txt', 'files/008.txt']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files:  ['files/001.txt', 'files/002.txt', 'files/003.txt', 'files/004.txt', 'files/005.txt', 'files/006.txt', 'files/007.txt', 'files/008.txt']\n",
      "Reading 'files/001.txt'...\n",
      "Reading 'files/002.txt'...\n",
      "Reading 'files/003.txt'...\n",
      "Reading 'files/004.txt'...\n",
      "Reading 'files/005.txt'...\n",
      "Reading 'files/006.txt'...\n",
      "Reading 'files/007.txt'...\n",
      "Reading 'files/008.txt'...\n"
     ]
    }
   ],
   "source": [
    "#Read files\n",
    "conv_filename = sorted(glob.glob('files/*.txt'))\n",
    "print(\"Found files: \", conv_filename)\n",
    "\n",
    "sonResponseDictionary = dict()\n",
    "for conv_filename in conv_filename:\n",
    "    print(\"Reading '{0}'...\".format(conv_filename))\n",
    "    with codecs.open(conv_filename, \"r\", \"utf-8\") as conv_file:\n",
    "        allLines = conv_file.readlines()\n",
    "        while '\\n' in allLines: allLines.remove('\\n')\n",
    "        #convert to everything to lowercase\n",
    "        [x.lower() for x in allLines]\n",
    "        \n",
    "        # FOR SON\n",
    "        personName = \"son\"\n",
    "        \n",
    "        myMessage, otherPersonsMessage, currentSpeaker = \"\",\"\",\"\"\n",
    "        for index,lines in enumerate(allLines):\n",
    "            justMessage = lines\n",
    "            colon = justMessage.find(':')\n",
    "            # Find messages of the current person\n",
    "            if (justMessage[:colon] == personName):\n",
    "                if not myMessage:\n",
    "                    startMessageIndex = index - 1\n",
    "                myMessage += justMessage[colon+1:]\n",
    "                if index == 1:\n",
    "                    prevMessage = allLines[0]\n",
    "                    colon = prevMessage.find(':')\n",
    "                    otherPersonsMessage = prevMessage[colon+1:]\n",
    "                    otherPersonsMessage = cleanMessage(otherPersonsMessage)\n",
    "                    myMessage = cleanMessage(myMessage)\n",
    "                    sonResponseDictionary[otherPersonsMessage] = myMessage\n",
    "                    \n",
    "            elif myMessage:\n",
    "                # Now go and see what message the other person sent by looking at previous messages\n",
    "                for counter in range(startMessageIndex, -1, -1):\n",
    "                    currentLine = allLines[counter]\n",
    "                    justMessage = currentLine\n",
    "                    colon = justMessage.find(':')\n",
    "                    if not currentSpeaker:\n",
    "                        # Other speaker\n",
    "                        currentSpeaker = justMessage[:colon]\n",
    "                    elif (currentSpeaker != justMessage[:colon] and otherPersonsMessage):\n",
    "                        # A different person started speaking, so now I know that the first person's message is done\n",
    "                        otherPersonsMessage = cleanMessage(otherPersonsMessage)\n",
    "                        myMessage = cleanMessage(myMessage)\n",
    "                        sonResponseDictionary[otherPersonsMessage] = myMessage\n",
    "                        break\n",
    "                    otherPersonsMessage = justMessage[colon+1:] + otherPersonsMessage\n",
    "                myMessage, otherPersonsMessage, currentSpeaker = \"\",\"\",\"\"\n",
    "        if myMessage:\n",
    "            for counter in range(startMessageIndex, -1, -1):\n",
    "                    currentLine = allLines[counter]\n",
    "                    justMessage = currentLine\n",
    "                    colon = justMessage.find(':')\n",
    "                    if not currentSpeaker:\n",
    "                        # Other speaker\n",
    "                        currentSpeaker = justMessage[:colon]\n",
    "                    elif (currentSpeaker != justMessage[:colon] and otherPersonsMessage):\n",
    "                        # A different person started speaking, so now I know that the first person's message is done\n",
    "                        otherPersonsMessage = cleanMessage(otherPersonsMessage)\n",
    "                        myMessage = cleanMessage(myMessage)\n",
    "                        sonResponseDictionary[otherPersonsMessage] = myMessage\n",
    "                        break\n",
    "                    otherPersonsMessage = justMessage[colon+1:] + otherPersonsMessage\n",
    "\n",
    "#Read files\n",
    "conv_filename = sorted(glob.glob('files/*.txt'))\n",
    "# print(\"Found files: \", conv_filename)\n",
    "\n",
    "fatherResponseDictionary = dict()\n",
    "for conv_filename in conv_filename:\n",
    "#     print(\"Reading '{0}'...\".format(conv_filename))\n",
    "    with codecs.open(conv_filename, \"r\", \"utf-8\") as conv_file:\n",
    "        allLines = conv_file.readlines()\n",
    "        while '\\n' in allLines: allLines.remove('\\n')\n",
    "        #convert to everything to lowercase\n",
    "        [x.lower() for x in allLines]\n",
    "        \n",
    "        # FOR FATHER\n",
    "        personName = \"father\"\n",
    "        \n",
    "        myMessage, otherPersonsMessage, currentSpeaker = \"\",\"\",\"\"\n",
    "        for index,lines in enumerate(allLines):\n",
    "            justMessage = lines\n",
    "            colon = justMessage.find(':')\n",
    "            # Find messages of the current person\n",
    "            if (justMessage[:colon] == personName):\n",
    "                if not myMessage:\n",
    "                    startMessageIndex = index - 1\n",
    "                myMessage += justMessage[colon+1:]\n",
    "                if index == 1:\n",
    "                    prevMessage = allLines[0]\n",
    "                    colon = prevMessage.find(':')\n",
    "                    otherPersonsMessage = prevMessage[colon+1:]\n",
    "                    otherPersonsMessage = cleanMessage(otherPersonsMessage)\n",
    "                    myMessage = cleanMessage(myMessage)\n",
    "                    fatherResponseDictionary[otherPersonsMessage] = myMessage\n",
    "                    \n",
    "            elif myMessage:\n",
    "                # Now go and see what message the other person sent by looking at previous messages\n",
    "                for counter in range(startMessageIndex, -1, -1):\n",
    "                    currentLine = allLines[counter]\n",
    "                    justMessage = currentLine\n",
    "                    colon = justMessage.find(':')\n",
    "                    if not currentSpeaker:\n",
    "                        # Other speaker\n",
    "                        currentSpeaker = justMessage[:colon]\n",
    "                    elif (currentSpeaker != justMessage[:colon] and otherPersonsMessage):\n",
    "                        # A different person started speaking, so now I know that the first person's message is done\n",
    "                        otherPersonsMessage = cleanMessage(otherPersonsMessage)\n",
    "                        myMessage = cleanMessage(myMessage)\n",
    "                        fatherResponseDictionary[otherPersonsMessage] = myMessage\n",
    "                        break\n",
    "                    otherPersonsMessage = justMessage[colon+1:] + otherPersonsMessage\n",
    "                myMessage, otherPersonsMessage, currentSpeaker = \"\",\"\",\"\"\n",
    "        if myMessage:\n",
    "            for counter in range(startMessageIndex, -1, -1):\n",
    "                    currentLine = allLines[counter]\n",
    "                    justMessage = currentLine\n",
    "                    colon = justMessage.find(':')\n",
    "                    if not currentSpeaker:\n",
    "                        # Other speaker\n",
    "                        currentSpeaker = justMessage[:colon]\n",
    "                    elif (currentSpeaker != justMessage[:colon] and otherPersonsMessage):\n",
    "                        # A different person started speaking, so now I know that the first person's message is done\n",
    "                        otherPersonsMessage = cleanMessage(otherPersonsMessage)\n",
    "                        myMessage = cleanMessage(myMessage)\n",
    "                        fatherResponseDictionary[otherPersonsMessage] = myMessage\n",
    "                        break\n",
    "                    otherPersonsMessage = justMessage[colon+1:] + otherPersonsMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sonResponseDictionary)\n",
    "# len(fatherResponseDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn words into tokens\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#remove unnnecessary,, split into words, no hyphens\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processDataset(allLines):\n",
    "    myStr = \"\"\n",
    "    for line in allLines:\n",
    "        myStr += line\n",
    "    myStr = re.sub(\"[^a-zA-Z]\",\" \", myStr)\n",
    "    finalDict = Counter(myStr.split())\n",
    "    return myStr, finalDict\n",
    "\n",
    "fullCorpus, datasetDictionary = processDataset(corpus_raw)\n",
    "print('Finished parsing and cleaning dataset')\n",
    "wordList = list(datasetDictionary.keys())\n",
    "\n",
    "with open(\"data/wordList.txt\", \"wb\") as fp:\n",
    "\tpickle.dump(wordList, fp)\n",
    "print(len(wordList), 'unique words found')\n",
    "print('Created file data/wordList.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)\n",
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "\n",
    "# print(raw_sentences[890])\n",
    "# print(sentence_to_wordlist(raw_sentences[890]))\n",
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The chat corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 0\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1\n",
    "\n",
    "chat2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "chat2vec.build_vocab(sentences)\n",
    "\n",
    "print(\"Word2Vec vocabulary length:\", len(chat2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat2vec.train(sentences, total_examples=chat2vec.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/embeddingMatrix.npy', chat2vec.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
